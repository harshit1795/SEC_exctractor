{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "# # Using SEC EDGAR RESTful data APIs\n",
    "# \n",
    "# This notebook shows how to retrieve information reported by regulated entities to U.S. Securities and Exchange Commision (SEC).\n",
    "# \n",
    "# SEC is maintainig EDGAR system with information about all regulated enties (companies, funds, individuals). Accessing the data is free and there is number of [various ways how to access the data](https://www.sec.gov/os/accessing-edgar-data).\n",
    "# \n",
    "# \"data.sec.gov\" was created to host RESTful data Application Programming Interfaces (APIs) delivering JSON-formatted data to external customers and to web pages on SEC.gov. These APIs do not require any authentication or API keys to access.\n",
    "# \n",
    "# Currently included in the APIs are the submissions history by filer and the XBRL data from financial statements (forms 10-Q, 10-K,8-K, 20-F, 40-F, 6-K, and their variants).\n",
    "# \n",
    "# The JSON structures are updated throughout the day, in real time, as submissions are disseminated.\n",
    "\n",
    "# pip install -r requirements.txt\n",
    "\n",
    "# !jupyter nbextension enable --py widgetsnbextension\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import re\n",
    "import unicodedata\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import warnings\n",
    "import boto3\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ## Finding CIK of company\n",
    "# \n",
    "# EDGAR assigns to filers a unique numerical identifier, known as a Central Index Key (CIK), when they sign up to make filings to the SEC. CIK numbers remain unique to the filer; they are not recycled. \n",
    "# \n",
    "# List of all CIKs matched with entity name is available for download [(13 MB, text file)](https://www.sec.gov/Archives/edgar/cik-lookup-data.txt). Note that this list includes funds and individuals and is historically cumulative for company names. Thus a given CIK may be associated with multiple names in the case of company or fund name changes, and the list contains some entities that no longer file with the SEC.\n",
    "# \n",
    "# We will be using smaller (611 kB) JSON [kaggle dataset](https://www.kaggle.com/datasets/svendaj/sec-edgar-cik-ticker-exchange), which is sourcing data directly at EDGAR and is input for this notebook. This dataset contains only companies names, CIK, ticker and associated stock exchange.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Let's convert CIK JSON to pandas DataFrame\n",
    "# First load the data into python dictionary\n",
    "\n",
    "\n",
    "CIK_df=pd.read_json(\"company_tickers.json\").T\n",
    "\n",
    "\n",
    "CIK_df.head()\n",
    "\n",
    "\n",
    "CIK_df.rename(columns={'cik_str': 'cik', 'title':'name'}, inplace=True)\n",
    "\n",
    "# ### Finding a particular company based upon the Name they are registered with\n",
    "\n",
    "\n",
    "# finding companies containing substring in company name\n",
    "substring = \"Black\"\n",
    "CIK_df[CIK_df[\"name\"].str.contains(substring, case=False)]\n",
    "\n",
    "\n",
    "# # Entity’s current filing history\n",
    "# \n",
    "# Each entity’s current filing history is available at the following URL:\n",
    "# \n",
    "# * https://data.sec.gov/submissions/CIK##########.json\n",
    "# \n",
    "# Where the ########## is the entity’s 10-digit Central Index Key (CIK), including leading zeros.\n",
    "# \n",
    "# This JSON data structure contains metadata such as current name, former name, and stock exchanges and ticker symbols of publicly-traded companies. The object’s property path contains at least one year’s of filing or to 1,000 (whichever is more) of the most recent filings in a compact columnar data array. If the entity has additional filings, files will contain an array of additional JSON files and the date range for the filings each one contains.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# read response from REST API with `requests` library and format it as python dict\n",
    "\n",
    "import requests\n",
    "header_full = {\n",
    "    \"User-Agent\": \"harshit harshit.gola.off@gmail.com\",\n",
    "    \"Accept-Encoding\": \"gzip, deflate\",\n",
    "    \"Host\": \"data.sec.gov\"\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "header = {\n",
    "    \"User-Agent\": \"harshit harshit.gola.off@gmail.com\",\n",
    "}\n",
    "\n",
    "\n",
    "# ## Select the ticker of company used in this example\n",
    "# \n",
    "# Subsequent information retrieval will be using selected `ticker` and associated CIK\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# finding company row with given ticker\n",
    "\n",
    "def get_current_filing_history(url, header):\n",
    "    company_filings = requests.get(url, headers=header).json()\n",
    "    company_filings_df = pd.DataFrame(company_filings[\"filings\"][\"recent\"])\n",
    "    return company_filings_df\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "BLK_CIK_df = CIK_df[CIK_df['ticker'] == 'BLK']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "BLK_CIK_df.head()\n",
    "\n",
    "\n",
    "# ## Reading from RESTful API\n",
    "# \n",
    "# EDGAR requires that HTTP requests will be identified with proper [UserAgent in header and comply with fair use policy (currently max. 10 requests per second)](https://www.sec.gov/os/accessing-edgar-data). At minimum you need to supply your own e-mail adress in User-Agent field (otherwise you will get 403/Forbiden error). If you will provide Host field, please be sure use data.sec.gov server and not www.sec.gov as mentioned in example (this would result in 404/Not Found error).\n",
    "\n",
    "# ## Creating DataFrame with submitted filings\n",
    "# \n",
    "# `company_filings[\"filings\"][\"recent\"]` contains up to 1000 last submitted filings sorted from latest to oldest.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def pull_all_history(df, header):\n",
    "    df_=pd.DataFrame()\n",
    "    for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "        CIK = row['cik']\n",
    "        url = f\"https://data.sec.gov/submissions/CIK{str(CIK).zfill(10)}.json\"\n",
    "        company_filings_df = get_current_filing_history(url, header)\n",
    "        company_filings_df['ticker']=row['ticker']\n",
    "        company_filings_df['cik']=row['cik']\n",
    "        df_ = pd.concat([company_filings_df, df_])\n",
    "    return df_\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_history = pull_all_history(CIK_df[:10], header_full)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_history_blk = pull_all_history(BLK_CIK_df, header_full)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def search_dataframe(dataframe, keyword):\n",
    "\n",
    "    # Create an empty DataFrame to store the matched rows\n",
    "    matched_rows = pd.DataFrame()\n",
    "    \n",
    "    # Iterate over each column in the DataFrame\n",
    "    for column in dataframe.columns:\n",
    "        # Convert non-string columns to strings\n",
    "        if not dataframe[column].dtype == 'object':\n",
    "            dataframe[column] = dataframe[column].astype(str)\n",
    "        \n",
    "        # Use str.contains to search for the keyword in each column\n",
    "        matched = dataframe[dataframe[column].str.contains(keyword, case=False, na=False)]\n",
    "        \n",
    "        # Concatenate the matched rows to the DataFrame\n",
    "        matched_rows = pd.concat([matched_rows, matched], ignore_index=True)\n",
    "    \n",
    "    return matched_rows\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "keyword = 'BlackRock'\n",
    "\n",
    "result = search_dataframe(df_history_blk, keyword)\n",
    "print(result)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_history.head()\n",
    "\n",
    "\n",
    "# ## Accessing specific filing document\n",
    "# \n",
    "# Let's download latest Annual Report (10-K). Files are stored in browsable directory structure for CIK and accession-number: \n",
    "# * https://www.sec.gov/Archives/edgar/data/{CIK}/{accession-number}/\n",
    "\n",
    "# Creating a function to create a url and run loop for all the items to download each of the filing htm file\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def download_all_forms(df, form, header):\n",
    "    df_ = df[df.form == form]\n",
    "    for index, row in tqdm(df_.iterrows(), total=df_.shape[0]):\n",
    "        url = f\"https://www.sec.gov/Archives/edgar/data/{row['cik']}/{row['accessionNumber'].replace('-', '')}/{row['primaryDocument']}\"\n",
    "        req_content = requests.get(url, headers=header).content.decode(\"utf-8\")\n",
    "        directory = f\"data/{row['ticker']}\"\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "\n",
    "        with open(f\"{directory}/{row['primaryDocument']}\", \"w\") as f:\n",
    "            f.write(req_content)\n",
    "\n",
    "\n",
    "# This step is to download all the 10K htm files for the 100 most recent filings into the data folder.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "download_all_forms(df_history, '10-K', header)\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# # 10-K form\n",
    "# ## Business, Risk, and MD&A\n",
    "# The function *parse_10k_filing()* parses 10-K forms to extract the following sections: business description, business risk, and management discussioin and analysis. The function takes two arguments, a link and a number indicating the section, and returns a list with the requested sections. Current options are **0(All), 1(Business), 2(Risk), 4(MDA).**\n",
    "# \n",
    "# Caveats:\n",
    "# The function *parse_10k_filing()* is a parser. You need to feed a SEC text link into it. There are many python and r packages to get a direct link to the fillings.\n",
    "# \n",
    "import re\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "def parse_10k_filing(file_path, section):\n",
    "    \n",
    "    if section not in [0, 1, 2, 3]:\n",
    "        print(\"Not a valid section\")\n",
    "        sys.exit()\n",
    "    \n",
    "    def get_text(file_path):\n",
    "        with open(file_path, 'r') as file:\n",
    "            content = file.read()\n",
    "        html = bs(content, 'html.parser')\n",
    "        text = html.get_text()\n",
    "        text = unicodedata.normalize(\"NFKD\", text).encode('ascii', 'ignore').decode('utf8')\n",
    "        text = text.split(\"\\n\")\n",
    "        text = \" \".join(text)\n",
    "        return text\n",
    "    \n",
    "    def extract_text(text, item_start, item_end):\n",
    "        item_start = item_start\n",
    "        item_end = item_end\n",
    "        starts = [i.start() for i in item_start.finditer(text)]\n",
    "        ends = [i.start() for i in item_end.finditer(text)]\n",
    "        positions = list()\n",
    "        for s in starts:\n",
    "            control = 0\n",
    "            for e in ends:\n",
    "                if control == 0:\n",
    "                    if s < e:\n",
    "                        control = 1\n",
    "                        positions.append([s,e])\n",
    "        item_length = 0\n",
    "        item_position = list()\n",
    "        for p in positions:\n",
    "            if (p[1]-p[0]) > item_length:\n",
    "                item_length = p[1]-p[0]\n",
    "                item_position = p\n",
    "\n",
    "        item_text = text[item_position[0]:item_position[1]]\n",
    "\n",
    "        return item_text\n",
    "\n",
    "    text = get_text(file_path)\n",
    "        \n",
    "    if section == 1 or section == 0:\n",
    "        try:\n",
    "            item1_start = re.compile(\"item\\s*[1][\\.\\;\\:\\-\\_]*\\s*\\\\b\", re.IGNORECASE)\n",
    "            item1_end = re.compile(\"item\\s*1a[\\.\\;\\:\\-\\_]\\s*Risk|item\\s*2[\\.\\,\\;\\:\\-\\_]\\s*Prop\", re.IGNORECASE)\n",
    "            businessText = extract_text(text, item1_start, item1_end)\n",
    "        except:\n",
    "            businessText = \"Something went wrong!\"\n",
    "        \n",
    "    if section == 2 or section == 0:\n",
    "        try:\n",
    "            item1a_start = re.compile(\"(?<!,\\s)item\\s*1a[\\.\\;\\:\\-\\_]\\s*Risk\", re.IGNORECASE)\n",
    "            item1a_end = re.compile(\"item\\s*2[\\.\\;\\:\\-\\_]\\s*Prop|item\\s*[1][\\.\\;\\:\\-\\_]*\\s*\\\\b\", re.IGNORECASE)\n",
    "            riskText = extract_text(text, item1a_start, item1a_end)\n",
    "        except:\n",
    "            riskText = \"Something went wrong!\"\n",
    "            \n",
    "    if section == 3 or section == 0:\n",
    "        try:\n",
    "            item7_start = re.compile(\"item\\s*[7][\\.\\;\\:\\-\\_]*\\s*\\\\bM\", re.IGNORECASE)\n",
    "            item7_end = re.compile(\"item\\s*7a[\\.\\;\\:\\-\\_]\\sQuanti|item\\s*8[\\.\\,\\;\\:\\-\\_]\\s*\", re.IGNORECASE)\n",
    "            mdaText = extract_text(text, item7_start, item7_end)\n",
    "        except:\n",
    "            mdaText = \"Something went wrong!\"\n",
    "    \n",
    "    if section == 0:\n",
    "        data = [businessText, riskText, mdaText]\n",
    "    elif section == 1:\n",
    "        data = [businessText]\n",
    "    elif section == 2:\n",
    "        data = [riskText]\n",
    "    elif section == 3:\n",
    "        data = [mdaText]\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def parse_all_forms(df, form, header):\n",
    "    df_ = df[df.form == form]\n",
    "    df__ = pd.DataFrame()\n",
    "    for index, row in tqdm(df_.iterrows(), total=df_.shape[0]):\n",
    "        directory = f\"data/{row['ticker']}\"\n",
    "        file_path = f\"data/{row['ticker']}/{row['primaryDocument']}\"\n",
    "        section = 0\n",
    "        \n",
    "        # Parse the 10-K filing and store the results in a DataFrame\n",
    "        text_data = parse_10k_filing(file_path, section)\n",
    "        df_text = pd.DataFrame({'Text': text_data})\n",
    "        df_text['ticker'] = row['ticker']\n",
    "        df_text['filepath'] = file_path\n",
    "        df__ = pd.concat([df_text, df__])\n",
    "\n",
    "    return df__\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_text = parse_all_forms(df_history , '10-K', header)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_history[df_history['ticker']=='GOOGL']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Exporting the resulted dataframe in a csv format.\n",
    "\n",
    "file_path='./All Reports Data.csv'\n",
    "\n",
    "\n",
    "\n",
    "#Arranging the index and adding Id column\n",
    "df_text = df_text.reset_index(drop=True)\n",
    "df_text['Id'] = df_text.index+1\n",
    "\n",
    "# Pop the column from its current position\n",
    "new_column = df_text.pop('Id')\n",
    "\n",
    "# Insert the column at the front of the DataFrame\n",
    "df_text.insert(0, 'Id', new_column)\n",
    "\n",
    "df_text.to_csv(file_path, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
